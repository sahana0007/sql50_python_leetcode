{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9bba90-8565-43f8-8781-c1897db93328",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|customer_id|customer_name|\n+-----------+-------------+\n|          1|       Daniel|\n|          2|        Diana|\n|          3|    Elizabeth|\n|          4|         Jhon|\n+-----------+-------------+\n\n+--------+-----------+------------+\n|order_id|customer_id|product_name|\n+--------+-----------+------------+\n|      10|          1|           A|\n|      20|          1|           B|\n|      30|          1|           D|\n|      40|          1|           C|\n|      50|          2|           A|\n|      60|          3|           A|\n|      70|          3|           B|\n|      80|          3|           D|\n|      90|          4|           C|\n+--------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreatePySparkTables\").getOrCreate()\n",
    "\n",
    "# Define the data schema for the Customers table\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the data schema for the Orders table\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrames from the provided data\n",
    "customers_data = [(1, \"Daniel\"), (2, \"Diana\"), (3, \"Elizabeth\"), (4, \"Jhon\")]\n",
    "orders_data = [(10, 1, \"A\"), (20, 1, \"B\"), (30, 1, \"D\"), (40, 1, \"C\"),\n",
    "               (50, 2, \"A\"), (60, 3, \"A\"), (70, 3, \"B\"), (80, 3, \"D\"), (90, 4, \"C\")]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Create temporary tables\n",
    "customers_df.createOrReplaceTempView(\"Customers\")\n",
    "orders_df.createOrReplaceTempView(\"Orders\")\n",
    "\n",
    "# Verify that the tables are created\n",
    "customers_df.show()\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49702ac8-73ce-42c7-95af-8e2ab6c3c9ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "You can't use sum directly on PySpark columns within a group aggregation. Instead, you should use conditional aggregation with when to calculate counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c9ab21-98f7-4094-a4b9-cfb07b5fa10c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|customer_id|customer_name|\n+-----------+-------------+\n|          3|    Elizabeth|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import IPython\n",
    "\n",
    "# Function to log errors and additional information\n",
    "def log_error_and_info(exception):\n",
    "    try:\n",
    "        # Get the notebook name\n",
    "        notebook_name = IPython.notebook.notebook_name\n",
    "\n",
    "        # Get the current time\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Format the current time as a string\n",
    "        current_time_str = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # Log the error message and additional information\n",
    "        logging.error(f\"Error in notebook: {notebook_name}\")\n",
    "        logging.error(f\"Time of error: {current_time_str}\")\n",
    "        logging.error(f\"Error message: {str(exception)}\")\n",
    "\n",
    "        # Optionally, log the traceback for more detailed error information\n",
    "        logging.error(\"Traceback:\\n\" + traceback.format_exc())\n",
    "\n",
    "    except Exception as e:\n",
    "        # If there is an error while logging, print the error message\n",
    "        print(f\"An error occurred while logging: {str(e)}\")\n",
    "\n",
    "def recommendation_query():\n",
    "    try:\n",
    "        # Create a SparkSession\n",
    "        spark = SparkSession.builder.appName(\"RecommendationQuery\").getOrCreate()\n",
    "\n",
    "        # Assuming you have already created DataFrames customers_df and orders_df\n",
    "        # representing the Customers and Orders tables, respectively.\n",
    "\n",
    "        # Calculate the counts of purchases for each product\n",
    "        product_counts_df = orders_df.groupBy(\"customer_id\") \\\n",
    "            .agg(\n",
    "                sum(when(col(\"product_name\") == \"A\", 1).otherwise(0)).alias(\"A_count\"),\n",
    "                sum(when(col(\"product_name\") == \"B\", 1).otherwise(0)).alias(\"B_count\"),\n",
    "                sum(when(col(\"product_name\") == \"C\", 1).otherwise(0)).alias(\"C_count\")\n",
    "            )\n",
    "\n",
    "        # Filter customers who bought products A and B but not C\n",
    "        result_df = product_counts_df.filter(\n",
    "            (col(\"A_count\") > 0) & (col(\"B_count\") > 0) & (col(\"C_count\") == 0)\n",
    "        ) \\\n",
    "        .select(\"customer_id\") \\\n",
    "        .join(customers_df, \"customer_id\", \"inner\") \\\n",
    "        .orderBy(\"customer_id\")\n",
    "\n",
    "        # Show the result\n",
    "        result_df.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Call the log_error_and_info function to log error details\n",
    "        log_error_and_info(e)\n",
    "\n",
    "# Call the function to execute the query\n",
    "recommendation_query()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba418843-97ee-434f-8366-5e5c93ddb2ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients Table:\n+----------+------------+------------+\n|patient_id|patient_name|  conditions|\n+----------+------------+------------+\n|         1|      Daniel|  YFEV COUGH|\n|         2|       Alice|            |\n|         3|         Bob|DIAB100 MYOP|\n|         4|      George|ACNE DIAB100|\n|         5|       Alain|     DIAB201|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PatientsTable\").getOrCreate()\n",
    "\n",
    "# Sample data for the Patients table\n",
    "data = [(1, \"Daniel\", \"YFEV COUGH\"),\n",
    "        (2, \"Alice\", \"\"),\n",
    "        (3, \"Bob\", \"DIAB100 MYOP\"),\n",
    "        (4, \"George\", \"ACNE DIAB100\"),\n",
    "        (5, \"Alain\", \"DIAB201\")]\n",
    "\n",
    "# Create a DataFrame for the Patients table\n",
    "patients_df = spark.createDataFrame(data, [\"patient_id\", \"patient_name\", \"conditions\"])\n",
    "\n",
    "# Show the contents of the Patients table\n",
    "print(\"Patients Table:\")\n",
    "patients_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a7ebc8-137a-4842-ac75-d7693867b043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients with Type I Diabetes (DIAB1 prefix):\n+----------+------------+------------+\n|patient_id|patient_name|  conditions|\n+----------+------------+------------+\n|         3|         Bob|DIAB100 MYOP|\n|         4|      George|ACNE DIAB100|\n+----------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter patients with Type I Diabetes (DIAB1 prefix)\n",
    "type1_diabetes_patients_df = patients_df.filter(col(\"conditions\").like(\"DIAB1%\") | col(\"conditions\").like(\"% DIAB1%\"))\n",
    "\n",
    "# Select the required columns\n",
    "result_df = type1_diabetes_patients_df.select(\"patient_id\", \"patient_name\", \"conditions\")\n",
    "\n",
    "# Show the patients with Type I Diabetes\n",
    "print(\"Patients with Type I Diabetes (DIAB1 prefix):\")\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NOT C",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
